# Configurações para CPU LLM Project
# Este é um arquivo de exemplo. Copie para .env e ajuste os valores conforme necessário.

# Configurações do Modelo e Engine
NUM_THREADS=0           # Número de threads para o Llama Engine. 0 = automático (baseado nos núcleos da CPU, com limite).
MODEL_N_CTX=2048        # Tamanho máximo do contexto do modelo.
DEFAULT_MODEL_PATH=""   # Caminho padrão para o arquivo .gguf do modelo (opcional, se não fornecido via CLI).

# Parâmetros de Amostragem (usados quando a geração de texto avançada for restaurada)
MODEL_TEMPERATURE=0.8
MODEL_TOP_K=40
MODEL_TOP_P=0.9
MODEL_REPEAT_PENALTY=1.1
# MAX_TOKENS_TO_GENERATE=128 # Pode ser definido aqui também, ou via API/parâmetro de função

# System Prompt (usado quando a geração de texto for restaurada e integrada)
SYSTEM_PROMPT="Você é um assistente de IA prestativo e conciso."

# Configurações do Servidor API (se o modo servidor for ativado)
API_HOST="localhost"
API_PORT=8080
